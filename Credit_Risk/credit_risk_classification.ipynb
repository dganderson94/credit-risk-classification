{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Much of the original starter file has been removed, to slim down the end product; this is in line with the rubric, which states to follow DRY principles and use concise, relevant notes. Every resource I've found on the matter of good commenting practices insists that the code should be almost entirely self-explanatory, with as few comments as possible. I have adhered to this.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_evaluation(X_train, X_test, y_train, y_test):\n",
    "    model = LogisticRegression(random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Balanced accuracy score not required here by rubric; left for use in analysis\n",
    "    print(f\"\"\"\\\n",
    "Balanced Accuracy Score:\n",
    "{balanced_accuracy_score(y_test, predictions)}\n",
    "\n",
    "Confusion Matrix:\n",
    "{confusion_matrix(predictions, y_test)}\n",
    "\n",
    "Classification Report:\n",
    "{classification_report(predictions, y_test)}\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Split the Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df = pd.read_csv(Path('Resources/lending_data.csv'))\n",
    "\n",
    "y = loan_df[\"loan_status\"]\n",
    "X = loan_df.drop(columns=\"loan_status\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Create a Logistic Regression Model with the Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy Score:\n",
      "0.9520479254722232\n",
      "\n",
      "Confusion Matrix:\n",
      "[[18663    56]\n",
      " [  102   563]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     18719\n",
      "           1       0.91      0.85      0.88       665\n",
      "\n",
      "    accuracy                           0.99     19384\n",
      "   macro avg       0.95      0.92      0.94     19384\n",
      "weighted avg       0.99      0.99      0.99     19384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_evaluation(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How well does the logistic regression model predict both the `0` (healthy loan) and `1` (high-risk loan) labels?\n",
    "\n",
    "**Answer:** The model predicts healthy loans with a precision of 99% and high-risk loans with a precision of 91%."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Predict a Logistic Regression Model with Resampled Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy Score:\n",
      "0.9936781215845847\n",
      "\n",
      "Confusion Matrix:\n",
      "[[18649     4]\n",
      " [  116   615]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     18653\n",
      "           1       0.99      0.84      0.91       731\n",
      "\n",
      "    accuracy                           0.99     19384\n",
      "   macro avg       0.99      0.92      0.95     19384\n",
      "weighted avg       0.99      0.99      0.99     19384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This entire section is missing from the rubric, which feels like an error, so I left it in.\n",
    "ros = RandomOverSampler(random_state=1)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "logistic_regression_evaluation(X_resampled, X_test, y_resampled, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How well does the logistic regression model, fit with oversampled data, predict both the `0` (healthy loan) and `1` (high-risk loan) labels?\n",
    "\n",
    "**Answer:** The model predicts both healthy loans and high-risk loans with a precision of 99%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
